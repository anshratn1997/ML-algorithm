{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this funtion is to fetch all file name and put in to a list x \n",
    "# y value is corresponding to newgroup which vary from 0 to 19 to represent all 20 newsgroup\n",
    "def get_data(path):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    i=0\n",
    "    for newspaper in os.listdir(path):\n",
    "        n_path=path+'/'+newspaper\n",
    "        for filename in os.listdir(n_path):\n",
    "            new_path=n_path+'/'+filename\n",
    "            x.append(new_path)\n",
    "            y.append(i)\n",
    "        i+=1\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is to remove punctuation and then tokenize it \n",
    "def get_tokens(doc):\n",
    "        deletechars={ord(c): None for c in string.punctuation} # to remove punctuation\n",
    "        no_punctuation=doc.translate(deletechars)\n",
    "        tokens=nltk.word_tokenize(no_punctuation)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this funtion is to tokenize all document into a token_data and also put tokens for every \n",
    "# individual newsgroup into a seperate list \n",
    "def tokenization(x_train,y_train):\n",
    "    token_data=[]\n",
    "    token_list=[[] for i in range(20)]\n",
    "    for i in range(len(x_train)):\n",
    "        file=open(x_train[i],'r')\n",
    "        doc=file.read()\n",
    "        lowers=doc.lower()\n",
    "        token=get_tokens(lowers)\n",
    "        filter_token=[w for w in token if not w in stopset]\n",
    "        token_data.extend(filter_token)\n",
    "        token_list[y_train[i]].extend(filter_token)\n",
    "    return token_data,token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this funtion is to create an 2d np array of 20 * 20001 to store frequency of feature words for individual newsgroup\n",
    "def get_dictionary(token_list,feature_name):\n",
    "    x=np.zeros((20,2000),dtype=int)\n",
    "    for i in range(20):\n",
    "        token_data=token_list[i]\n",
    "        for word in token_data:\n",
    "            if word in feature_name:\n",
    "                j=feature_name.index(word)\n",
    "                x[i][j]+=1\n",
    "    total=[]\n",
    "    for i in range(20):\n",
    "        sum=0\n",
    "        for j in range(2000):\n",
    "            sum+=x[i][j]\n",
    "        total.append(sum)\n",
    "    t_arr=np.array(total)\n",
    "    t_arr=t_arr.reshape(len(t_arr),1) \n",
    "    x=np.append(x,t_arr,axis=1)    # to add a column in array which store sum of all words for every individual newspaper group\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is to select top 2000 words from dictionary\n",
    "def get_feature_name(count):\n",
    "    feature_name=[word for (word, freq) in count.most_common(2000)]\n",
    "    return feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is to calculate probablity of word occurence with lapalce correction\n",
    "def probablity(dictionary,x,feature_name,current_class):\n",
    "    output=np.log(1)-np.log(20)\n",
    "    occurance={}\n",
    "    for word in x:\n",
    "        if word in occurance.keys():\n",
    "            continue\n",
    "        else:\n",
    "            occurance[word]=1\n",
    "        if word in feature_name:\n",
    "            index=feature_name.index(word)\n",
    "            temp=np.log(dictionary[current_class][index]+1)-np.log(dictionary[current_class][dictionary.shape[1]-1]+2000)\n",
    "            output+=temp\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is to tokenize word for a file \n",
    "def tokenize_word(path):\n",
    "    file=open(path,'r')\n",
    "    doc=file.read()\n",
    "    lowers=doc.lower()\n",
    "    token=get_tokens(lowers)\n",
    "    filter_token=[w for w in token if not w in stopset]\n",
    "    return filter_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is to provide y predicted value\n",
    "def predict(dictionary,feature_name,x_test):\n",
    "    y_pred=[]\n",
    "    for i in range(len(x_test)):\n",
    "        words=tokenize_word(x_test[i])\n",
    "        best_p=-1e9\n",
    "        best_class=-1\n",
    "        for j in range(20):\n",
    "            current=probablity(dictionary,words,feature_name,j)\n",
    "            if(current>best_p):\n",
    "                best_p=current\n",
    "                best_class=j \n",
    "        y_pred.append(best_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path='../datasets/20_newsgroups'\n",
    "x,y=get_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,shuffle=True,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_data,token_list=tokenization(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=Counter(token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_name=get_feature_name(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary=get_dictionary(token_list,feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=predict(dictionary,feature_name,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.75      0.73       237\n",
      "          1       0.80      0.77      0.78       237\n",
      "          2       0.87      0.81      0.84       271\n",
      "          3       0.88      0.82      0.85       247\n",
      "          4       0.86      0.91      0.88       253\n",
      "          5       0.97      0.83      0.89       237\n",
      "          6       0.75      0.88      0.81       256\n",
      "          7       0.80      0.93      0.86       260\n",
      "          8       0.83      0.95      0.88       255\n",
      "          9       0.90      0.96      0.93       246\n",
      "         10       0.99      0.87      0.92       240\n",
      "         11       0.95      0.83      0.88       255\n",
      "         12       0.70      0.92      0.80       259\n",
      "         13       0.89      0.90      0.89       276\n",
      "         14       0.90      0.90      0.90       253\n",
      "         15       0.98      0.99      0.98       243\n",
      "         16       0.72      0.83      0.77       244\n",
      "         17       0.94      0.81      0.87       242\n",
      "         18       0.71      0.53      0.60       241\n",
      "         19       0.67      0.54      0.59       248\n",
      "\n",
      "avg / total       0.84      0.84      0.83      5000\n",
      "\n",
      "[[177   1   0   0   0   0   0   3   3   0   1   1   3   3   3   2   0   2\n",
      "    3  35]\n",
      " [  0 182   7   2   8   4   4   0   7   0   0   0  12   4   3   0   0   0\n",
      "    1   3]\n",
      " [  0   7 219  13   4   0   3   6   1   0   0   1  11   1   1   0   0   0\n",
      "    1   3]\n",
      " [  0   3   6 203   9   1  10   4   0   0   0   0   9   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   2   2 230   0   9   2   0   0   0   0   4   1   1   0   0   0\n",
      "    0   0]\n",
      " [  1  10  11   4   1 196   4   1   3   0   0   0   6   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   4   4   0 224   9   1   1   0   0   7   3   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   7 243   3   0   0   0   4   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   0   0   0   5   4 241   0   0   0   1   0   0   0   0   0\n",
      "    2   0]\n",
      " [  0   0   0   0   1   0   4   0   4 235   1   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   3   1   2  20 208   0   3   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0   5   3   0   1   2   2   1   5   0   1 211  12   1   2   0   6   0\n",
      "    2   1]\n",
      " [  0   1   0   3   4   0   6   5   0   0   0   0 238   1   1   0   0   0\n",
      "    0   0]\n",
      " [  3   3   2   0   0   0   4   1   4   0   0   1   8 248   2   0   0   0\n",
      "    0   0]\n",
      " [  0   7   0   0   0   0   0   1   3   1   0   0   5   4 227   0   1   0\n",
      "    4   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0 241   1   0\n",
      "    0   0]\n",
      " [  0   0   1   0   2   0   6   5   7   1   0   3   1   0   0   0 202   0\n",
      "    7   9]\n",
      " [  6   1   0   0   0   0   2   6   3   1   0   3   5   1   2   0   1 196\n",
      "   12   3]\n",
      " [  7   0   0   1   1   0   3   8   3   3   0   2   3   6   4   0  50  10\n",
      "  127  13]\n",
      " [ 55   1   0   0   0   0   3   4   0   0   0   0   4   2   3   4  19   0\n",
      "   20 133]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
